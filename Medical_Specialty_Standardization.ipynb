{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byq8IYNaJB6L"
      },
      "source": [
        "# Medical Specialty Standardization System\n",
        "\n",
        "This notebook implements a comprehensive system to standardize healthcare provider specialties against the NUCC (National Uniform Claim Committee) taxonomy. It uses multiple matching strategies including exact matching, fuzzy matching, and semantic similarity to map raw specialty text to standardized codes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGSKJno6JB6N"
      },
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "We start by importing all necessary libraries for data processing, machine learning, and text matching."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz sentence-transformers torch scikit-learn pandas numpy\n",
        "!pip install rapidfuzz sentence-transformers torch scikit-learn pandas numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z0HkXBYJNUb",
        "outputId": "c96e8358-cd63-49c6-98ba-89cc599258dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (3.14.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.12/dist-packages (3.14.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M35cWEIdJB6N",
        "outputId": "88b52684-2c24-441a-8375-4885ea8a5325"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from rapidfuzz import fuzz\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "print(\"✓ All imports successful\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All imports successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbOUATOhJB6O"
      },
      "source": [
        "## Step 2: Define Comprehensive Medical Abbreviations Map\n",
        "\n",
        "This dictionary maps common medical abbreviations to their full specialty names. This is crucial for preprocessing raw specialty text that contains abbreviated forms like 'cardio' → 'cardiology', 'obgyn' → 'obstetrics and gynecology', etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv5MGGOJJB6O",
        "outputId": "d608dcdd-97ec-4cbc-ec50-97fcb9d2f9e3"
      },
      "source": [
        "MEDICAL_ABBREVIATIONS = {\n",
        "    r\"\\bcardio\\b\": \"cardiology\",\n",
        "    r\"\\bcard\\b\": \"cardiology\",\n",
        "    r\"\\bcv\\b\": \"cardiovascular\",\n",
        "    r\"\\bcvs\\b\": \"cardiovascular\",\n",
        "    r\"\\bent\\b\": \"otolaryngology\",\n",
        "    r\"\\bento\\b\": \"otolaryngology\",\n",
        "    r\"\\bot\\b\": \"otolaryngology\",\n",
        "    r\"\\bsurg\\b\": \"surgery\",\n",
        "    r\"\\bcardiothoracic\\b\": \"cardiac surgery\",\n",
        "    r\"\\bthracic\\b\": \"thoracic surgery\",\n",
        "    r\"\\bobgyn\\b\": \"obstetrics and gynecology\",\n",
        "    r\"\\bob-gyn\\b\": \"obstetrics and gynecology\",\n",
        "    r\"\\bobs\\b\": \"obstetrics\",\n",
        "    r\"\\bgyn\\b\": \"gynecology\",\n",
        "    r\"\\burol\\b\": \"urology\",\n",
        "    r\"\\buro\\b\": \"urology\",\n",
        "    r\"\\bortho\\b\": \"orthopedics\",\n",
        "    r\"\\borthopaedic\\b\": \"orthopedics\",\n",
        "    r\"\\borthopedic\\b\": \"orthopedics\",\n",
        "    r\"\\bpsych\\b\": \"psychiatry\",\n",
        "    r\"\\bpsy\\b\": \"psychiatry\",\n",
        "    r\"\\bneuro\\b\": \"neurology\",\n",
        "    r\"\\bneuro surg\\b\": \"neurological surgery\",\n",
        "    r\"\\bderma\\b\": \"dermatology\",\n",
        "    r\"\\bderm\\b\": \"dermatology\",\n",
        "    r\"\\bpath\\b\": \"pathology\",\n",
        "    r\"\\blap path\\b\": \"laboratory pathology\",\n",
        "    r\"\\brad\\b\": \"radiology\",\n",
        "    r\"\\bradiotherapy\\b\": \"radiation therapy\",\n",
        "    r\"\\bicu\\b\": \"critical care medicine\",\n",
        "    r\"\\bccu\\b\": \"cardiac care\",\n",
        "    r\"\\bcritical\\b\": \"critical care medicine\",\n",
        "    r\"\\bpedi\\b\": \"pediatrics\",\n",
        "    r\"\\bped\\b\": \"pediatrics\",\n",
        "    r\"\\bpediatric\\b\": \"pediatrics\",\n",
        "    r\"\\bim\\b\": \"internal medicine\",\n",
        "    r\"\\bim doc\\b\": \"internal medicine\",\n",
        "    r\"\\bpt\\b\": \"physical therapy\",\n",
        "    r\"\\bphysical med\\b\": \"physical medicine and rehabilitation\",\n",
        "    r\"\\bmd\\b\": \"medical doctor\",\n",
        "    r\"\\brn\\b\": \"registered nurse\",\n",
        "    r\"\\blpn\\b\": \"licensed practical nurse\",\n",
        "    r\"\\bpa\\b\": \"physician assistant\",\n",
        "    r\"\\bnp\\b\": \"nurse practitioner\",\n",
        "}\n",
        "\n",
        "print(f\"✓ Loaded {len(MEDICAL_ABBREVIATIONS)} abbreviation mappings\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Loaded 44 abbreviation mappings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5N6mSciJB6P"
      },
      "source": [
        "## Step 3: Define Enums and Data Classes\n",
        "\n",
        "We use enums to track the matching method used, and dataclasses to structure the match results. This provides type safety and clarity about what data each match operation produces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG7-PP3XJB6Q",
        "outputId": "fea123f5-42fe-4fa6-de39-2c8f87be4e01"
      },
      "source": [
        "class MatchMethod(Enum):\n",
        "    \"\"\"Enum to track which matching strategy was used\"\"\"\n",
        "    EXACT_MATCH = \"exact_match\"\n",
        "    FUZZY_MATCH = \"fuzzy_match\"\n",
        "    SEMANTIC_MATCH = \"semantic_match\"\n",
        "    FALLBACK_MATCH = \"fallback_match\"\n",
        "    NO_MATCH = \"no_match\"\n",
        "    EMPTY_INPUT = \"empty_input\"\n",
        "\n",
        "@dataclass\n",
        "class MatchResult:\n",
        "    \"\"\"Dataclass to hold results of a specialty matching operation\"\"\"\n",
        "    primary_code: str\n",
        "    primary_confidence: float\n",
        "    calibrated_confidence: float\n",
        "    method: MatchMethod\n",
        "    is_multi_specialty: bool\n",
        "    alternatives: List[Tuple[str, float]]\n",
        "\n",
        "print(\"✓ MatchMethod and MatchResult defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ MatchMethod and MatchResult defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMaoQbfPJB6Q"
      },
      "source": [
        "## Step 4: Create the Specialty Preprocessor\n",
        "\n",
        "The preprocessor handles:\n",
        "- **Null/empty value handling**: Returns empty string for invalid inputs\n",
        "- **ID removal**: Removes NUCC codes in various formats\n",
        "- **Lowercasing**: Converts to lowercase for consistent matching\n",
        "- **Abbreviation expansion**: Uses our abbreviation map to expand shortened forms\n",
        "- **Character normalization**: Handles slashes, hyphens, underscores, special characters\n",
        "- **Stopword removal**: Removes common non-informative words like 'service', 'center', 'clinic'\n",
        "- **Misspelling correction**: Fixes common typos in medical terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6k-nY7vJB6Q",
        "outputId": "fd1c4641-8655-4660-b1df-5f6e85875888"
      },
      "source": [
        "class SpecialtyPreprocessor:\n",
        "    \"\"\"Preprocesses raw specialty text for matching\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.abbreviation_map = MEDICAL_ABBREVIATIONS\n",
        "\n",
        "    def preprocess(self, text: str) -> Tuple[str, bool]:\n",
        "        \"\"\"\n",
        "        Main preprocessing function.\n",
        "        Returns: (cleaned_text, is_compound_specialty)\n",
        "        \"\"\"\n",
        "        # Handle null and empty values\n",
        "        if pd.isna(text) or text == '':\n",
        "            return '', False\n",
        "\n",
        "        text = str(text).strip()\n",
        "        if len(text) < 2:\n",
        "            return '', False\n",
        "\n",
        "        # Remove NUCC codes (format: 10 alphanumeric chars, optionally ending with X)\n",
        "        text = re.sub(r'\\s*-\\s*[0-9A-Z]{10}X?\\s*$', '', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'^[0-9A-Z]{10}X?\\s*-\\s*', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Expand abbreviations\n",
        "        for abbrev_pattern, expansion in self.abbreviation_map.items():\n",
        "            text = re.sub(abbrev_pattern, expansion, text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Normalize special characters\n",
        "        text = re.sub(r'[/&]', ' and ', text)\n",
        "        text = re.sub(r'[\\-_]', ' ', text)\n",
        "        text = re.sub(r'[,()]', ' ', text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = {'service', 'center', 'clinic', 'hospital', 'department',\n",
        "                      'medical', 'healthcare', 'provider', 'physician', 'doctor',\n",
        "                      'general', 'office', 'practice', 'specialty', 'specialization'}\n",
        "        words = text.split()\n",
        "        words = [w for w in words if w not in stop_words and len(w) > 1]\n",
        "        text = ' '.join(words)\n",
        "\n",
        "        # Fix common misspellings\n",
        "        text = self._fix_common_misspellings(text)\n",
        "\n",
        "        # Clean up extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Detect if this is a compound specialty (contains 'and' or 3+ words)\n",
        "        is_compound = ' and ' in text or (len(text.split()) >= 3)\n",
        "\n",
        "        return text, is_compound\n",
        "\n",
        "    def _fix_common_misspellings(self, text: str) -> str:\n",
        "        \"\"\"Fix commonly misspelled medical terms\"\"\"\n",
        "        corrections = {\n",
        "            'clinal': 'clinical',\n",
        "            'cardiak': 'cardiac',\n",
        "            'diabetus': 'diabetes',\n",
        "            'ural': 'urology',\n",
        "            'oncolog': 'oncology',\n",
        "            'patho': 'pathology',\n",
        "            'radiolog': 'radiology',\n",
        "            'throacic': 'thoracic',\n",
        "            'neurolog': 'neurology',\n",
        "        }\n",
        "\n",
        "        for typo, correction in corrections.items():\n",
        "            text = re.sub(r'\\b' + typo + r'\\b', correction, text, flags=re.IGNORECASE)\n",
        "\n",
        "        return text\n",
        "\n",
        "# Test the preprocessor\n",
        "test_preprocessor = SpecialtyPreprocessor()\n",
        "test_cases = [\n",
        "    'Cardio Surgery - 2058367891X',\n",
        "    'obgyn & Obstetrics',\n",
        "    'neuro_surg(brain)',\n",
        "    'ENT/Otolaryngology Department',\n",
        "]\n",
        "\n",
        "print(\"\\n✓ Preprocessor test cases:\")\n",
        "for test in test_cases:\n",
        "    cleaned, is_compound = test_preprocessor.preprocess(test)\n",
        "    print(f\"  '{test}' → '{cleaned}' (compound: {is_compound})\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Preprocessor test cases:\n",
            "  'Cardio Surgery - 2058367891X' → 'cardiology surgery' (compound: False)\n",
            "  'obgyn & Obstetrics' → 'obstetrics and gynecology and obstetrics' (compound: True)\n",
            "  'neuro_surg(brain)' → 'neuro surg brain' (compound: True)\n",
            "  'ENT/Otolaryngology Department' → 'otolaryngology and otolaryngology' (compound: True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lw1JkWuJB6Q"
      },
      "source": [
        "## Step 5: Create the Specialty Matcher\n",
        "\n",
        "The matcher implements a multi-strategy approach:\n",
        "1. **Exact Match**: Perfect or near-perfect matches\n",
        "2. **Fuzzy Match**: Handles typos and variations (using Levenshtein distance)\n",
        "3. **Semantic Match**: Uses sentence embeddings to find conceptually similar specialties\n",
        "4. **Multi-Specialty Match**: Handles combined specialties like 'cardiology and internal medicine'\n",
        "5. **Fallback Match**: When no good matches found, returns lowest confidence option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlGDJXVzJB6Q",
        "outputId": "69043b19-cee3-408d-b52e-e235b0ea77a4"
      },
      "source": [
        "class SpecialtyMatcher:\n",
        "    \"\"\"Matches specialty text against NUCC taxonomy using multiple strategies\"\"\"\n",
        "\n",
        "    def __init__(self, nucc_df: pd.DataFrame):\n",
        "        self.nucc_df = nucc_df.copy()\n",
        "        self.preprocessor = SpecialtyPreprocessor()\n",
        "\n",
        "        # Build lookup dictionaries\n",
        "        self.code_to_display = dict(zip(nucc_df['Code'], nucc_df['Display_Name']))\n",
        "        self.nucc_display_clean = [\n",
        "            self.preprocessor.preprocess(name)[0] for name in nucc_df['Display_Name']\n",
        "        ]\n",
        "\n",
        "        # Load semantic model for embeddings\n",
        "        try:\n",
        "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            self.nucc_embeddings = self.model.encode(self.nucc_display_clean, convert_to_tensor=True)\n",
        "            self.semantic_ready = True\n",
        "            print(\"✓ Semantic model loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Semantic model failed: {e}\")\n",
        "            self.semantic_ready = False\n",
        "\n",
        "    def match(self, specialty: str) -> MatchResult:\n",
        "        \"\"\"Main matching function - tries multiple strategies in order\"\"\"\n",
        "        cleaned, is_compound = self.preprocessor.preprocess(specialty)\n",
        "\n",
        "        # Check for empty input\n",
        "        if not cleaned or len(cleaned) < 2:\n",
        "            return MatchResult(\n",
        "                primary_code='JUNK',\n",
        "                primary_confidence=0.0,\n",
        "                calibrated_confidence=0.0,\n",
        "                method=MatchMethod.EMPTY_INPUT,\n",
        "                is_multi_specialty=False,\n",
        "                alternatives=[]\n",
        "            )\n",
        "\n",
        "        # Try exact match first (highest confidence)\n",
        "        exact_result = self._exact_match(cleaned)\n",
        "        if exact_result:\n",
        "            code, confidence = exact_result\n",
        "            return self._create_result(code, confidence, MatchMethod.EXACT_MATCH, is_compound, cleaned)\n",
        "\n",
        "        # Try fuzzy match (good for typos)\n",
        "        fuzzy_result = self._fuzzy_match(cleaned)\n",
        "        if fuzzy_result and fuzzy_result[1] >= 0.85:\n",
        "            code, confidence = fuzzy_result\n",
        "            return self._create_result(code, confidence, MatchMethod.FUZZY_MATCH, is_compound, cleaned)\n",
        "\n",
        "        # Try semantic match (good for paraphrasing)\n",
        "        if self.semantic_ready:\n",
        "            semantic_result = self._semantic_match(cleaned)\n",
        "            if semantic_result and semantic_result[1] >= 0.50:\n",
        "                code, confidence = semantic_result\n",
        "                return self._create_result(code, confidence, MatchMethod.SEMANTIC_MATCH, is_compound, cleaned)\n",
        "\n",
        "        # Try multi-specialty match for compound inputs\n",
        "        if is_compound and ' and ' in cleaned:\n",
        "            multi_result = self._multi_specialty_match(cleaned)\n",
        "            if multi_result:\n",
        "                code, confidence = multi_result\n",
        "                return self._create_result(code, confidence, MatchMethod.SEMANTIC_MATCH, True, cleaned)\n",
        "\n",
        "        # Fallback: use fuzzy match with low confidence\n",
        "        if fuzzy_result:\n",
        "            code, confidence = fuzzy_result\n",
        "            confidence = min(confidence, 0.45)\n",
        "            return self._create_result(code, confidence, MatchMethod.FALLBACK_MATCH, is_compound, cleaned)\n",
        "\n",
        "        # No match found at all\n",
        "        return MatchResult(\n",
        "            primary_code='JUNK',\n",
        "            primary_confidence=0.0,\n",
        "            calibrated_confidence=0.0,\n",
        "            method=MatchMethod.NO_MATCH,\n",
        "            is_multi_specialty=is_compound,\n",
        "            alternatives=[]\n",
        "        )\n",
        "\n",
        "    def _exact_match(self, cleaned: str) -> Optional[Tuple[str, float]]:\n",
        "        \"\"\"Check for exact or near-exact matches\"\"\"\n",
        "        for i, nucc_clean in enumerate(self.nucc_display_clean):\n",
        "            if nucc_clean == cleaned or (nucc_clean in cleaned and len(cleaned) > 5):\n",
        "                if fuzz.ratio(cleaned, nucc_clean) > 95:\n",
        "                    code = self.nucc_df.iloc[i]['Code']\n",
        "                    return code, 0.98\n",
        "        return None\n",
        "\n",
        "    def _fuzzy_match(self, cleaned: str) -> Optional[Tuple[str, float]]:\n",
        "        \"\"\"Fuzzy matching using token-based Levenshtein distance\"\"\"\n",
        "        best_code = None\n",
        "        best_score = 0\n",
        "\n",
        "        for i, nucc_clean in enumerate(self.nucc_display_clean):\n",
        "            score = fuzz.token_set_ratio(cleaned, nucc_clean) / 100.0\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_code = self.nucc_df.iloc[i]['Code']\n",
        "\n",
        "        if best_score >= 0.70:\n",
        "            return best_code, best_score\n",
        "        return None\n",
        "\n",
        "    def _semantic_match(self, cleaned: str) -> Optional[Tuple[str, float]]:\n",
        "        \"\"\"Semantic matching using transformer embeddings\"\"\"\n",
        "        if not self.semantic_ready:\n",
        "            return None\n",
        "\n",
        "        input_embedding = self.model.encode(cleaned, convert_to_tensor=True)\n",
        "        similarities = util.pytorch_cos_sim(input_embedding, self.nucc_embeddings)[0]\n",
        "\n",
        "        best_idx = torch.argmax(similarities).item()\n",
        "        best_score = float(similarities[best_idx])\n",
        "\n",
        "        if best_score >= 0.40:\n",
        "            code = self.nucc_df.iloc[best_idx]['Code']\n",
        "            return code, best_score\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _get_top_alternatives(self, cleaned: str, top_n: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Get top N alternative matches for reporting\"\"\"\n",
        "        if not self.semantic_ready:\n",
        "            return []\n",
        "\n",
        "        input_embedding = self.model.encode(cleaned, convert_to_tensor=True)\n",
        "        similarities = util.pytorch_cos_sim(input_embedding, self.nucc_embeddings)[0]\n",
        "\n",
        "        top_scores, top_indices = torch.topk(similarities, k=min(top_n + 1, len(similarities)))\n",
        "\n",
        "        alternatives = []\n",
        "        for idx, score in zip(top_indices.tolist(), top_scores.tolist()):\n",
        "            code = self.nucc_df.iloc[idx]['Code']\n",
        "            if score >= 0.35:\n",
        "                alternatives.append((code, float(score)))\n",
        "\n",
        "        return alternatives\n",
        "\n",
        "    def _multi_specialty_match(self, cleaned: str) -> Optional[Tuple[str, float]]:\n",
        "        \"\"\"Handle compound specialties like 'cardiology and internal medicine'\"\"\"\n",
        "        parts = [p.strip() for p in re.split(r'\\s+and\\s+', cleaned)]\n",
        "        if len(parts) < 2:\n",
        "            return None\n",
        "\n",
        "        part_matches = []\n",
        "        for part in parts:\n",
        "            if len(part) < 3:\n",
        "                continue\n",
        "            result = self._semantic_match(part)\n",
        "            if result:\n",
        "                part_matches.append(result)\n",
        "\n",
        "        if not part_matches:\n",
        "            return None\n",
        "\n",
        "        best_code, best_conf = max(part_matches, key=lambda x: x[1])\n",
        "        return best_code, best_conf * 0.95\n",
        "\n",
        "    def _create_result(self, code: str, confidence: float, method: MatchMethod,\n",
        "                     is_compound: bool, cleaned: str) -> MatchResult:\n",
        "        \"\"\"Create a result object with alternatives\"\"\"\n",
        "        alternatives = self._get_top_alternatives(cleaned, top_n=5)\n",
        "        alternatives = [(c, s) for c, s in alternatives if c != code and s < confidence]\n",
        "\n",
        "        return MatchResult(\n",
        "            primary_code=code,\n",
        "            primary_confidence=confidence,\n",
        "            calibrated_confidence=confidence,\n",
        "            method=method,\n",
        "            is_multi_specialty=is_compound,\n",
        "            alternatives=alternatives\n",
        "        )\n",
        "\n",
        "print(\"✓ SpecialtyMatcher class defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SpecialtyMatcher class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AESo6wkJB6R"
      },
      "source": [
        "## Step 6: Create the Confidence Calibrator\n",
        "\n",
        "This class uses isotonic regression to calibrate raw confidence scores into well-calibrated probabilities. This means if the model says 0.80 confidence, it should be correct approximately 80% of the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQZECHrWJB6R",
        "outputId": "aa69aac1-ea57-4156-9881-64cb7038786f"
      },
      "source": [
        "class ConfidenceCalibrator:\n",
        "    \"\"\"Calibrates raw confidence scores to true probabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, original_scores: np.ndarray, ground_truth: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the calibrator using validation data\n",
        "\n",
        "        Args:\n",
        "            original_scores: Raw confidence scores from the matcher\n",
        "            ground_truth: Binary labels (1 if correct match, 0 if incorrect)\n",
        "        \"\"\"\n",
        "        original_scores = np.array(original_scores).flatten()\n",
        "        ground_truth = np.array(ground_truth).flatten()\n",
        "\n",
        "        self.iso_reg.fit(original_scores, ground_truth)\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def calibrate(self, scores: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply calibration to new scores\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            return scores\n",
        "        return self.iso_reg.predict(scores)\n",
        "\n",
        "print(\"✓ ConfidenceCalibrator class defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ConfidenceCalibrator class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRAueZ_OJB6R"
      },
      "source": [
        "## Step 7: Create the Junk Classifier\n",
        "\n",
        "This classifier determines whether a match result should be classified as 'JUNK' (unmappable). It uses threshold rules based on the matching method used to decide when to mark a result as unmappable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZTDHwJ0JB6R",
        "outputId": "9626eecb-4de4-4922-c4a0-b0c6e3a52c4d"
      },
      "source": [
        "class JunkClassifier:\n",
        "    \"\"\"Determines if a match should be classified as unmappable (JUNK)\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def should_classify_junk(result: MatchResult, raw_text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Determine if this match should be marked as JUNK.\n",
        "        Uses different thresholds based on the matching method.\n",
        "        \"\"\"\n",
        "        # Empty input is always junk\n",
        "        if result.method == MatchMethod.EMPTY_INPUT:\n",
        "            return True\n",
        "\n",
        "        # Very short text is likely junk\n",
        "        if len(raw_text.strip()) < 2:\n",
        "            return True\n",
        "\n",
        "        # Apply method-specific thresholds\n",
        "        if result.method == MatchMethod.EXACT_MATCH:\n",
        "            return result.primary_confidence < 0.95\n",
        "        elif result.method == MatchMethod.FUZZY_MATCH:\n",
        "            return result.primary_confidence < 0.80\n",
        "        elif result.method == MatchMethod.SEMANTIC_MATCH:\n",
        "            return result.primary_confidence < 0.50\n",
        "        elif result.method == MatchMethod.FALLBACK_MATCH:\n",
        "            return result.primary_confidence < 0.35\n",
        "        elif result.method == MatchMethod.NO_MATCH:\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "print(\"✓ JunkClassifier class defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ JunkClassifier class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lAdzr6aJB6S"
      },
      "source": [
        "## Step 8: Create the Main Standardizer\n",
        "\n",
        "This is the main orchestrator class that:\n",
        "1. Takes raw specialty data\n",
        "2. Runs it through the matcher\n",
        "3. Applies junk classification\n",
        "4. Optionally calibrates confidences\n",
        "5. Returns formatted results with alternatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZdpeKFvJB6S",
        "outputId": "cb10092c-89af-4022-f741-3bbb1ffa6d30"
      },
      "source": [
        "class ProviderSpecialtyStandardizer:\n",
        "    \"\"\"Main orchestrator for specialty standardization\"\"\"\n",
        "\n",
        "    def __init__(self, nucc_df: pd.DataFrame):\n",
        "        self.nucc_df = nucc_df\n",
        "        self.matcher = SpecialtyMatcher(nucc_df)\n",
        "        self.calibrator = ConfidenceCalibrator()\n",
        "\n",
        "    def standardize(self, input_df: pd.DataFrame,\n",
        "                   specialty_column: str = 'raw_specialty',\n",
        "                   apply_calibration: bool = False) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Main standardization function.\n",
        "\n",
        "        Args:\n",
        "            input_df: DataFrame with specialty data\n",
        "            specialty_column: Name of the column containing specialties (default: 'raw_specialty')\n",
        "            apply_calibration: Whether to apply learned calibration\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with standardized specialties\n",
        "        \"\"\"\n",
        "        # Verify column exists\n",
        "        if specialty_column not in input_df.columns:\n",
        "            raise KeyError(f\"Column '{specialty_column}' not found. Available: {input_df.columns.tolist()}\")\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Process each specialty\n",
        "        for idx, row in input_df.iterrows():\n",
        "            specialty = row[specialty_column]\n",
        "            match_result = self.matcher.match(specialty)\n",
        "\n",
        "            # Apply junk classification\n",
        "            is_junk = JunkClassifier.should_classify_junk(match_result, specialty)\n",
        "\n",
        "            if is_junk:\n",
        "                match_result.primary_code = 'JUNK'\n",
        "                match_result.primary_confidence = 0.0\n",
        "                match_result.calibrated_confidence = 0.0\n",
        "            elif apply_calibration:\n",
        "                # Apply learned calibration if available\n",
        "                cal_score = self.calibrator.calibrate(\n",
        "                    np.array([match_result.primary_confidence])\n",
        "                )[0]\n",
        "                match_result.calibrated_confidence = cal_score\n",
        "            else:\n",
        "                # Apply simple calibration rules\n",
        "                match_result.calibrated_confidence = self._simple_calibrate(\n",
        "                    match_result.primary_confidence,\n",
        "                    match_result.method\n",
        "                )\n",
        "\n",
        "            results.append(match_result)\n",
        "\n",
        "            # Progress reporting\n",
        "            if (idx + 1) % 1000 == 0:\n",
        "                print(f\"  Processed {idx + 1} records...\")\n",
        "\n",
        "        return self._format_results(input_df, results, specialty_column)\n",
        "\n",
        "    def _simple_calibrate(self, score: float, method: MatchMethod) -> float:\n",
        "        \"\"\"Apply simple calibration rules based on matching method\"\"\"\n",
        "        if method == MatchMethod.EXACT_MATCH:\n",
        "            return min(score * 1.02, 0.95)\n",
        "        elif method == MatchMethod.FUZZY_MATCH:\n",
        "            return min(score * 1.05, 0.90)\n",
        "        elif method == MatchMethod.SEMANTIC_MATCH:\n",
        "            return min(score ** 0.5, 0.85)\n",
        "        elif method == MatchMethod.FALLBACK_MATCH:\n",
        "            return min(score * 0.95, 0.50)\n",
        "        else:\n",
        "            return score\n",
        "\n",
        "    def _format_results(self, input_df: pd.DataFrame,\n",
        "                      results: List[MatchResult],\n",
        "                      specialty_column: str) -> pd.DataFrame:\n",
        "        \"\"\"Format results into a comprehensive output DataFrame\"\"\"\n",
        "        output_rows = []\n",
        "\n",
        "        for i, (idx, row) in enumerate(input_df.iterrows()):\n",
        "            result = results[i]\n",
        "            cleaned, _ = self.matcher.preprocessor.preprocess(row[specialty_column])\n",
        "\n",
        "            # Build output row\n",
        "            output_row = {\n",
        "                'Specialty': row[specialty_column],\n",
        "                'Preprocessed': cleaned,\n",
        "                'Primary_Code': result.primary_code,\n",
        "                'Original_Confidence': round(result.primary_confidence, 4),\n",
        "                'Calibrated_Confidence': round(result.calibrated_confidence, 4),\n",
        "                'Method': result.method.value,\n",
        "                'Is_Multi_Specialty': result.is_multi_specialty,\n",
        "            }\n",
        "\n",
        "            # Add alternative matches\n",
        "            for j, (alt_code, alt_score) in enumerate(result.alternatives[:5]):\n",
        "                output_row[f'Alternative_Code_{j+1}'] = alt_code\n",
        "                output_row[f'Alternative_Score_{j+1}'] = round(float(alt_score), 4)\n",
        "\n",
        "            output_rows.append(output_row)\n",
        "\n",
        "        output_df = pd.DataFrame(output_rows)\n",
        "\n",
        "        # Ensure all alternative columns exist\n",
        "        for j in range(1, 6):\n",
        "            if f'Alternative_Code_{j}' not in output_df.columns:\n",
        "                output_df[f'Alternative_Code_{j}'] = np.nan\n",
        "            output_df[f'Alternative_Score_{j}'] = np.nan\n",
        "\n",
        "        return output_df\n",
        "\n",
        "    def compute_validation_metrics(self, output_df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"\n",
        "        Compute comprehensive validation metrics for the standardization results.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with metrics including junk rate, success rate, confidence stats, etc.\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Basic counts\n",
        "        total = len(output_df)\n",
        "        junk_count = (output_df['Primary_Code'] == 'JUNK').sum()\n",
        "\n",
        "        metrics['total_records'] = total\n",
        "        metrics['junk_records'] = junk_count\n",
        "        metrics['mapped_records'] = total - junk_count\n",
        "        metrics['junk_percentage'] = round((junk_count / total) * 100, 2)\n",
        "        metrics['mapping_success_rate'] = round(((total - junk_count) / total) * 100, 2)\n",
        "\n",
        "        # Confidence metrics\n",
        "        non_junk = output_df[output_df['Primary_Code'] != 'JUNK']\n",
        "\n",
        "        metrics['avg_original_confidence'] = round(non_junk['Original_Confidence'].mean(), 4)\n",
        "        metrics['avg_calibrated_confidence'] = round(non_junk['Calibrated_Confidence'].mean(), 4)\n",
        "        metrics['confidence_improvement'] = round(\n",
        "            metrics['avg_calibrated_confidence'] - metrics['avg_original_confidence'], 4\n",
        "        )\n",
        "\n",
        "        # Method distribution\n",
        "        metrics['method_distribution'] = output_df['Method'].value_counts().to_dict()\n",
        "\n",
        "        # Confidence by method\n",
        "        metrics['confidence_by_method'] = {}\n",
        "        for method in output_df['Method'].unique():\n",
        "            method_data = non_junk[non_junk['Method'] == method]['Calibrated_Confidence']\n",
        "            if len(method_data) > 0:\n",
        "                metrics['confidence_by_method'][method] = round(method_data.mean(), 4)\n",
        "\n",
        "        # Multi-specialty metrics\n",
        "        multi = output_df[output_df['Is_Multi_Specialty'] == True]\n",
        "        if len(multi) > 0:\n",
        "            metrics['multi_specialty_count'] = len(multi)\n",
        "            metrics['multi_specialty_avg_confidence'] = round(\n",
        "                multi[multi['Primary_Code'] != 'JUNK']['Calibrated_Confidence'].mean(), 4\n",
        "            )\n",
        "\n",
        "        # Low confidence tracking\n",
        "        low_conf = non_junk[non_junk['Calibrated_Confidence'] < 0.60]\n",
        "        metrics['low_confidence_count'] = len(low_conf)\n",
        "        metrics['low_confidence_percentage'] = round((len(low_conf) / len(non_junk)) * 100, 2)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "print(\"✓ ProviderSpecialtyStandardizer class defined\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ProviderSpecialtyStandardizer class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To6TIRdrJB6S"
      },
      "source": [
        "## Step 9: Load Data and Initialize Standardizer\n",
        "\n",
        "Load the NUCC taxonomy master file and the input specialties data, then initialize the standardizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks96MlY2JB6S",
        "outputId": "134b30f5-c8bb-4eb1-ef79-3a81c0f48752"
      },
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load your data\n",
        "nucc_df = pd.read_csv('nucc_taxonomy_master.csv')\n",
        "input_df = pd.read_csv('input_specialties.csv')\n",
        "\n",
        "print(f\"✓ NUCC records: {len(nucc_df)}\")\n",
        "print(f\"✓ Input specialties: {len(input_df)}\")\n",
        "print(f\"✓ Input columns: {input_df.columns.tolist()}\")\n",
        "\n",
        "# Create standardizer\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INITIALIZING STANDARDIZER\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "standardizer = ProviderSpecialtyStandardizer(nucc_df)\n",
        "print(\"\\n✓ Standardizer initialized and ready\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "LOADING DATA\n",
            "==================================================\n",
            "✓ NUCC records: 879\n",
            "✓ Input specialties: 10050\n",
            "✓ Input columns: ['raw_specialty']\n",
            "\n",
            "==================================================\n",
            "INITIALIZING STANDARDIZER\n",
            "==================================================\n",
            "✓ Semantic model loaded\n",
            "\n",
            "✓ Standardizer initialized and ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwJeZjgbJB6S"
      },
      "source": [
        "## Step 10: Run Standardization\n",
        "\n",
        "Execute the standardization process on all input specialties. This will match each specialty against the NUCC taxonomy and assign codes with confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyoMYgJ3JB6S",
        "outputId": "f4080553-4f28-4cb6-a6b6-a043308ee151"
      },
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RUNNING STANDARDIZATION\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Run standardization with correct column name\n",
        "output_df = standardizer.standardize(\n",
        "    input_df,\n",
        "    specialty_column='raw_specialty'  # <-- CORRECT COLUMN NAME\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Standardization completed\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "RUNNING STANDARDIZATION\n",
            "==================================================\n",
            "\n",
            "  Processed 1000 records...\n",
            "  Processed 2000 records...\n",
            "  Processed 3000 records...\n",
            "  Processed 4000 records...\n",
            "  Processed 5000 records...\n",
            "  Processed 6000 records...\n",
            "  Processed 7000 records...\n",
            "  Processed 8000 records...\n",
            "  Processed 9000 records...\n",
            "  Processed 10000 records...\n",
            "\n",
            "✓ Standardization completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVHT5tQ3JB6T"
      },
      "source": [
        "## Step 11: Compute and Display Validation Metrics\n",
        "\n",
        "Compute comprehensive metrics showing how well the standardization performed, including success rates, confidence distributions, and method effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyEPWLbZJB6T",
        "outputId": "b5343500-531f-471c-ddd8-68142a8958f7"
      },
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALIDATION METRICS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Compute metrics\n",
        "metrics = standardizer.compute_validation_metrics(output_df)\n",
        "\n",
        "print(\"\\n=== CORE METRICS ===\")\n",
        "print(f\"Total records: {metrics['total_records']}\")\n",
        "print(f\"Successfully mapped: {metrics['mapped_records']} ({metrics['mapping_success_rate']}%)\")\n",
        "print(f\"Unmappable (JUNK): {metrics['junk_records']} ({metrics['junk_percentage']}%)\")\n",
        "\n",
        "print(\"\\n=== CONFIDENCE METRICS ===\")\n",
        "print(f\"Average original confidence: {metrics['avg_original_confidence']}\")\n",
        "print(f\"Average calibrated confidence: {metrics['avg_calibrated_confidence']}\")\n",
        "print(f\"Confidence improvement: {metrics['confidence_improvement']}\")\n",
        "\n",
        "print(\"\\n=== METHOD DISTRIBUTION ===\")\n",
        "for method, count in sorted(metrics['method_distribution'].items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{method}: {count}\")\n",
        "\n",
        "print(\"\\n=== CONFIDENCE BY METHOD ===\")\n",
        "for method, conf in sorted(metrics['confidence_by_method'].items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{method}: {conf}\")\n",
        "\n",
        "if 'multi_specialty_count' in metrics:\n",
        "    print(f\"\\nMulti-specialty matches: {metrics['multi_specialty_count']}\")\n",
        "    print(f\"Multi-specialty avg confidence: {metrics['multi_specialty_avg_confidence']}\")\n",
        "\n",
        "print(f\"\\nLow confidence matches (<0.60): {metrics['low_confidence_count']} ({metrics['low_confidence_percentage']}%)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "VALIDATION METRICS\n",
            "==================================================\n",
            "\n",
            "=== CORE METRICS ===\n",
            "Total records: 10050\n",
            "Successfully mapped: 9547 (95.0%)\n",
            "Unmappable (JUNK): 503 (5.0%)\n",
            "\n",
            "=== CONFIDENCE METRICS ===\n",
            "Average original confidence: 0.9575\n",
            "Average calibrated confidence: 0.909\n",
            "Confidence improvement: -0.0485\n",
            "\n",
            "=== METHOD DISTRIBUTION ===\n",
            "fuzzy_match: 4874\n",
            "exact_match: 3586\n",
            "semantic_match: 1043\n",
            "no_match: 401\n",
            "empty_input: 102\n",
            "fallback_match: 44\n",
            "\n",
            "=== CONFIDENCE BY METHOD ===\n",
            "exact_match: 0.95\n",
            "fuzzy_match: 0.9\n",
            "semantic_match: 0.8307\n",
            "fallback_match: 0.4275\n",
            "\n",
            "Multi-specialty matches: 3673\n",
            "Multi-specialty avg confidence: 0.9066\n",
            "\n",
            "Low confidence matches (<0.60): 44 (0.46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwwGFCqhJB6T"
      },
      "source": [
        "## Step 12: Save Standardized Results\n",
        "\n",
        "Save the standardized output to a CSV file with all matching details including the primary NUCC code, confidence scores, and alternative matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPhg9H60JB6T",
        "outputId": "76bbb4a8-0cf8-4b52-d338-383821e79ad2"
      },
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Save output\n",
        "output_df.to_csv('Output_detailed.csv', index=False)\n",
        "print(\"✓ Saved to Output_detailed.csv\")\n",
        "\n",
        "print(f\"\\nOutput shape: {output_df.shape}\")\n",
        "print(\"\\nFirst few results:\")\n",
        "print(output_df[['Specialty', 'Preprocessed', 'Primary_Code', 'Calibrated_Confidence', 'Method']].head(10))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "SAVING RESULTS\n",
            "==================================================\n",
            "✓ Saved to Output_detailed.csv\n",
            "\n",
            "Output shape: (10050, 17)\n",
            "\n",
            "First few results:\n",
            "                           Specialty                     Preprocessed  \\\n",
            "0                        ACUPUNCTURE                      acupuncture   \n",
            "1                ADOLESCENT MEDICINE              adolescent medicine   \n",
            "2               ALLERGY & IMMUNOLOGY           allergy and immunology   \n",
            "3      ANATOMIC & CLINICAL PATHOLOGY  anatomic and clinical pathology   \n",
            "4                     ANESTHESIOLOGY                   anesthesiology   \n",
            "5  APPLIED BEHAVIORAL ANALYSIS (ABA)  applied behavioral analysis aba   \n",
            "6                          AUDIOLOGY                        audiology   \n",
            "7                  BARIATRIC SURGERY                bariatric surgery   \n",
            "8          CARDIAC ELECTROPHYSIOLOGY        cardiac electrophysiology   \n",
            "9                    CARDIAC SURGERY                  cardiac surgery   \n",
            "\n",
            "  Primary_Code  Calibrated_Confidence          Method  \n",
            "0   171100000X                 0.8500  semantic_match  \n",
            "1   207QA0000X                 0.9000     fuzzy_match  \n",
            "2   207K00000X                 0.9500     exact_match  \n",
            "3   207ZP0101X                 0.9000     fuzzy_match  \n",
            "4   207L00000X                 0.9500     exact_match  \n",
            "5   103K00000X                 0.8177  semantic_match  \n",
            "6   2355A2700X                 0.9000     fuzzy_match  \n",
            "7   208600000X                 0.9000     fuzzy_match  \n",
            "8   207RC0001X                 0.9000     fuzzy_match  \n",
            "9   208600000X                 0.9000     fuzzy_match  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84GTgQMyJB6T"
      },
      "source": [
        "## Step 13: Create Explainable Output Format\n",
        "\n",
        "Create a simplified, pipe-separated output format with explainable rationale. This provides a user-friendly view showing:\n",
        "- Raw specialty input\n",
        "- NUCC codes (primary and alternatives)\n",
        "- Confidence scores\n",
        "- Simple explanation of the match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STudIzcBJB6T",
        "outputId": "a829b246-b54f-499b-b2f8-e2d922a4a558"
      },
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CREATING EXPLAINABLE OUTPUT\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "def create_explain_row(row: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Processes a single row from the main output_df to create the\n",
        "    pipe-separated 'explain' format.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Compile Codes and Confidences\n",
        "    # Start with the primary match\n",
        "    codes = [row['Primary_Code']]\n",
        "    confidences = [str(row['Calibrated_Confidence'])]\n",
        "\n",
        "    # Add alternatives, checking for NaNs\n",
        "    for i in range(1, 6):\n",
        "        alt_code = row[f'Alternative_Code_{i}']\n",
        "        alt_score = row[f'Alternative_Score_{i}']\n",
        "\n",
        "        if pd.notna(alt_code):\n",
        "            codes.append(str(alt_code))\n",
        "            confidences.append(str(round(float(alt_score), 4)))\n",
        "\n",
        "    # 2. Create the Rationale\n",
        "    explain_text = \"\"\n",
        "    if row['Primary_Code'] == 'JUNK':\n",
        "        explain_text = \"Input was empty, too short, or unmappable (JUNK).\"\n",
        "    else:\n",
        "        method = row['Method']\n",
        "        confidence = row['Calibrated_Confidence']\n",
        "        explain_text = f\"Mapped via {method} with confidence {confidence:.2f}.\"\n",
        "\n",
        "        if row['Is_Multi_Specialty']:\n",
        "            explain_text += \" (Detected multi-specialty input)\"\n",
        "\n",
        "    # 3. Return the new row as a Series\n",
        "    return pd.Series({\n",
        "        'raw_specialty': row['Specialty'],  # 'Specialty' holds the raw input\n",
        "        'nucc_codes': '|'.join(codes),\n",
        "        'confidence': '|'.join(confidences),\n",
        "        'explain': explain_text\n",
        "    })\n",
        "\n",
        "# Apply the function across the main output DataFrame\n",
        "explain_df = output_df.apply(create_explain_row, axis=1)\n",
        "\n",
        "# Save the new CSV\n",
        "explain_df.to_csv('Final_Submission_output.csv', index=False)\n",
        "\n",
        "print(\"✓ Saved to Final_Submission_output.csv\")\n",
        "print(f\"\\nExplain output shape: {explain_df.shape}\")\n",
        "print(\"\\nFirst few 'explain' results:\")\n",
        "print(explain_df.head(10))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "CREATING EXPLAINABLE OUTPUT\n",
            "==================================================\n",
            "\n",
            "✓ Saved to Final_Submission_output.csv\n",
            "\n",
            "Explain output shape: (10050, 4)\n",
            "\n",
            "First few 'explain' results:\n",
            "                       raw_specialty  \\\n",
            "0                        ACUPUNCTURE   \n",
            "1                ADOLESCENT MEDICINE   \n",
            "2               ALLERGY & IMMUNOLOGY   \n",
            "3      ANATOMIC & CLINICAL PATHOLOGY   \n",
            "4                     ANESTHESIOLOGY   \n",
            "5  APPLIED BEHAVIORAL ANALYSIS (ABA)   \n",
            "6                          AUDIOLOGY   \n",
            "7                  BARIATRIC SURGERY   \n",
            "8          CARDIAC ELECTROPHYSIOLOGY   \n",
            "9                    CARDIAC SURGERY   \n",
            "\n",
            "                                          nucc_codes  \\\n",
            "0  171100000X|208VP0000X|207LP2900X|2081P2900X|26...   \n",
            "1  207QA0000X|2080A0000X|207RA0000X|207QA0505X|20...   \n",
            "2  207K00000X|207KI0005X|207RA0201X|207KA0200X|20...   \n",
            "3  207ZP0101X|207ZP0102X|207ZC0006X|207ZP0105X|20...   \n",
            "4  207L00000X|1223D0004X|367H00000X|207LP3000X|20...   \n",
            "5  103K00000X|106E00000X|251S00000X|106S00000X|10...   \n",
            "6  2355A2700X|231H00000X|231HA2400X|231HA2500X|23...   \n",
            "7  208600000X|2086P0122X|208C00000X|207VB0002X|20...   \n",
            "8  207RC0001X|2251E1300X|2084N0600X|261QR0404X|20...   \n",
            "9  208600000X|208G00000X|2086S0129X|261QR0404X|20...   \n",
            "\n",
            "                   confidence  \\\n",
            "0    0.85|nan|nan|nan|nan|nan   \n",
            "1     0.9|nan|nan|nan|nan|nan   \n",
            "2    0.95|nan|nan|nan|nan|nan   \n",
            "3     0.9|nan|nan|nan|nan|nan   \n",
            "4    0.95|nan|nan|nan|nan|nan   \n",
            "5  0.8177|nan|nan|nan|nan|nan   \n",
            "6     0.9|nan|nan|nan|nan|nan   \n",
            "7     0.9|nan|nan|nan|nan|nan   \n",
            "8     0.9|nan|nan|nan|nan|nan   \n",
            "9     0.9|nan|nan|nan|nan|nan   \n",
            "\n",
            "                                             explain  \n",
            "0    Mapped via semantic_match with confidence 0.85.  \n",
            "1       Mapped via fuzzy_match with confidence 0.90.  \n",
            "2  Mapped via exact_match with confidence 0.95. (...  \n",
            "3  Mapped via fuzzy_match with confidence 0.90. (...  \n",
            "4       Mapped via exact_match with confidence 0.95.  \n",
            "5  Mapped via semantic_match with confidence 0.82...  \n",
            "6       Mapped via fuzzy_match with confidence 0.90.  \n",
            "7       Mapped via fuzzy_match with confidence 0.90.  \n",
            "8       Mapped via fuzzy_match with confidence 0.90.  \n",
            "9       Mapped via fuzzy_match with confidence 0.90.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priQnCX4JB6T"
      },
      "source": [
        "## Step 14: Summary and Next Steps\n",
        "\n",
        "### What You Now Have:\n",
        "\n",
        "1. **Output_detailed.csv** - Comprehensive output with:\n",
        "   - Original specialty text\n",
        "   - Preprocessed text\n",
        "   - Primary NUCC code with confidence\n",
        "   - Up to 5 alternative matches with scores\n",
        "   - Matching method used\n",
        "   - Multi-specialty flag\n",
        "\n",
        "2. **Final_Submission_output.csv** - Simple explainable format with:\n",
        "   - Raw input\n",
        "   - Pipe-separated NUCC codes (primary and alternatives)\n",
        "   - Pipe-separated confidence scores\n",
        "   - Plain English explanation\n",
        "\n",
        "### Key Metrics:\n",
        "- **Mapping Success Rate**: Percentage of records successfully mapped\n",
        "- **Junk Rate**: Unmappable records\n",
        "- **Confidence Scores**: Calibrated probabilities for each match\n",
        "- **Method Distribution**: Which matching strategies worked best\n",
        "\n",
        "### To Use This Notebook:\n",
        "1. Update the file paths in Step 9 to point to your data\n",
        "2. Ensure your input CSV has a column named `raw_specialty`\n",
        "3. Ensure your NUCC master CSV has columns `Code` and `Display_Name`\n",
        "4. Run cells sequentially from top to bottom"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}