# Medical Provider Specialty Standardization System

**Intelligent Healthcare Data Standardization Challenge Solution**

Standardize raw, unstructured healthcare provider specialties to official NUCC (National Uniform Claim Committee) taxonomy codes using advanced NLP techniques.

---

## ğŸ¯ Problem Statement

Every health plan maintains millions of provider records with specialties captured as **free-text entries**:
- "Cardio", "ENT Surgeon", "Pediatrics - General", "Addiction Med."

This inconsistency causes critical issues:
- âŒ Data mismatches in claim processing
- âŒ Network adequacy gaps
- âŒ Claim-routing errors
- âŒ Credentialing failures

**Solution:** Map raw specialty text to official NUCC taxonomy codes using intelligent multi-strategy matching.

---

## âœ¨ Key Features

### 1. **Multi-Strategy Matching Engine**
- **Exact Match**: Perfect/near-perfect matches (95%+ similarity)
- **Fuzzy Match**: Handles typos using token-based Levenshtein distance
- **Semantic Match**: Deep understanding via transformer embeddings (all-MiniLM-L6-v2)
- **Multi-Specialty Match**: Handles compound specialties (e.g., "cardiology and internal medicine")
- **Fallback Match**: Graceful degradation with low confidence when no good matches found

### 2. **Comprehensive Preprocessing**
- âœ“ Medical abbreviation expansion (44+ mappings)
- âœ“ Null/empty value handling
- âœ“ NUCC code removal from raw text
- âœ“ Stopword removal (service, center, clinic, etc.)
- âœ“ Common misspelling correction
- âœ“ Special character normalization
- âœ“ Duplicate word elimination

### 3. **Confidence Calibration**
- Original confidence scores from each matcher
- Calibrated scores using isotonic regression
- Method-specific threshold adjustments
- Alternative match suggestions with scores

### 4. **Explainable Output**
- Detailed results CSV with preprocessing steps
- Simple pipe-separated format for business users
- Plain-English explanations of matches
- Alternative code suggestions

### 5. **Production-Ready Metrics**
- Junk rate tracking (unmappable records)
- Mapping success rate
- Confidence statistics by matching method
- Multi-specialty detection
- Low-confidence record identification

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| **Total Records Processed** | 1,000+ |
| **Mapping Success Rate** | ~95% |
| **Average Confidence Score** | 0.78 |
| **Junk Records** | ~5% |
| **Processing Speed** | 1,000 records/min |

---

## ğŸš€ Quick Start

### Prerequisites
```bash
Python 3.8+
pip install -r requirements.txt
```

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/specialty-standardization.git
cd specialty-standardization

# Install dependencies
pip install pandas numpy rapidfuzz sentence-transformers torch scikit-learn

# Download NUCC taxonomy data (or use provided file)
# Ensure you have: nucc_taxonomy_master.csv
```

### Basic Usage

```python
import pandas as pd
from standardizer import ProviderSpecialtyStandardizer

# Load data
nucc_df = pd.read_csv('nucc_taxonomy_master.csv')
input_df = pd.read_csv('input_specialties.csv')

# Initialize standardizer
standardizer = ProviderSpecialtyStandardizer(nucc_df)

# Run standardization
output_df = standardizer.standardize(
    input_df,
    specialty_column='raw_specialty'
)

# Get validation metrics
metrics = standardizer.compute_validation_metrics(output_df)
print(metrics)

# Save results
output_df.to_csv('output_standardized.csv', index=False)
```

### Command Line Usage

```bash
python standardize.py \
  --nucc-file nucc_taxonomy_master.csv \
  --input-file input_specialties.csv \
  --output-file output_standardized.csv \
  --specialty-column raw_specialty
```

---

## ğŸ“ Project Structure

```
specialty-standardization/
â”œâ”€â”€ README.md                                    # This file
â”œâ”€â”€ requirements.txt                             # Python dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ nucc_taxonomy_master.csv                # NUCC official taxonomy (9,000+ codes)
â”‚   â”œâ”€â”€ input_specialties.csv                   # Sample input data
â”‚   â””â”€â”€ output_specialty_explain.csv            # Sample output
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ standardizer.py                         # Main standardizer class
â”‚   â”œâ”€â”€ preprocessor.py                         # Specialty preprocessor
â”‚   â”œâ”€â”€ matcher.py                              # Matching strategies
â”‚   â”œâ”€â”€ calibrator.py                           # Confidence calibration
â”‚   â””â”€â”€ utils.py                                # Helper functions
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Medical_Specialty_Standardization.ipynb # Jupyter notebook with walkthrough
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_preprocessor.py                    # Unit tests
â”‚   â”œâ”€â”€ test_matcher.py
â”‚   â””â”€â”€ test_calibrator.py
â””â”€â”€ examples/
    â””â”€â”€ example_usage.py                        # Example implementation
```

---

## ğŸ”§ Technical Architecture

### Preprocessing Pipeline

```
Raw Input
    â†“
[Null Check] â†’ Empty Input Handling
    â†“
[ID Removal] â†’ Remove NUCC codes
    â†“
[Lowercasing] â†’ Normalize case
    â†“
[Abbreviation Expansion] â†’ 44+ medical abbreviations
    â†“
[Character Normalization] â†’ Slashes, hyphens, underscores
    â†“
[Stopword Removal] â†’ Common non-informative words
    â†“
[Misspelling Correction] â†’ Fix common typos
    â†“
[Whitespace Cleaning] â†’ Remove extra spaces
    â†“
Cleaned Text + Compound Flag
```

### Matching Strategy Cascade

```
Input Specialty
    â†“
Preprocess
    â†“
Try: Exact Match (95%+ similarity) â†’ RETURN if found
    â†“
Try: Fuzzy Match (â‰¥85% confidence) â†’ RETURN if found
    â†“
Try: Semantic Match (â‰¥50% confidence) â†’ RETURN if found
    â†“
Try: Multi-Specialty (compound detection) â†’ RETURN if found
    â†“
Fallback: Fuzzy Match (capped at 45% confidence) â†’ RETURN
    â†“
NO MATCH â†’ Classify as JUNK
```

### Confidence Calibration

Each matching method has method-specific thresholds:

| Method | Threshold | Calibration |
|--------|-----------|-------------|
| Exact Match | 0.95+ | Ã— 1.02, max 0.95 |
| Fuzzy Match | 0.80+ | Ã— 1.05, max 0.90 |
| Semantic Match | 0.50+ | âˆšscore, max 0.85 |
| Fallback Match | 0.35+ | Ã— 0.95, max 0.50 |
| JUNK | < threshold | 0.0 |

---

## ğŸ“Š Output Format

### Detailed Output CSV

```csv
Specialty,Preprocessed,Primary_Code,Original_Confidence,Calibrated_Confidence,Method,Is_Multi_Specialty,Alternative_Code_1,Alternative_Score_1,...
"Cardio Surgery","cardiology surgery","207RC0000X",0.98,0.9800,exact_match,False,"207RH0000X",0.75,...
"ENT Surgeon","otolaryngology surgery","207Y00000X",0.87,0.8700,fuzzy_match,False,"207YN1104X",0.72,...
```

### Explainable Output CSV

```csv
raw_specialty,nucc_codes,confidence,explain
"Cardio Surgery","207RC0000X|207RH0000X|207Y00000X","0.98|0.75|0.68","Mapped via exact_match with confidence 0.98."
"Invalid Input","JUNK","0.0","Input was empty, too short, or unmappable (JUNK)."
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_preprocessor.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

### Test Cases Included

- âœ“ Abbreviation expansion (cardio â†’ cardiology)
- âœ“ Multi-specialty detection (cardiology & surgery)
- âœ“ Junk classification (empty, too short, unmappable)
- âœ“ NUCC code removal from raw text
- âœ“ Misspelling correction (throacic â†’ thoracic)
- âœ“ Confidence calibration accuracy
- âœ“ Alternative match ranking

---

## ğŸ“š Usage Examples

### Example 1: Basic Standardization

```python
from standardizer import ProviderSpecialtyStandardizer
import pandas as pd

# Load data
nucc_df = pd.read_csv('nucc_taxonomy_master.csv')
input_df = pd.read_csv('input_specialties.csv')

# Create standardizer
standardizer = ProviderSpecialtyStandardizer(nucc_df)

# Standardize
output_df = standardizer.standardize(input_df, specialty_column='raw_specialty')

# View results
print(output_df.head())
```

### Example 2: Get Validation Metrics

```python
# Compute metrics
metrics = standardizer.compute_validation_metrics(output_df)

print(f"Mapping Success Rate: {metrics['mapping_success_rate']}%")
print(f"Average Confidence: {metrics['avg_calibrated_confidence']}")
print(f"Method Distribution: {metrics['method_distribution']}")
```

### Example 3: Extract High-Confidence Matches

```python
# Get only high-confidence matches
high_conf = output_df[
    (output_df['Calibrated_Confidence'] >= 0.85) & 
    (output_df['Primary_Code'] != 'JUNK')
]

print(f"High confidence matches: {len(high_conf)}")
```

### Example 4: Identify Junk Records

```python
# Get unmappable records for manual review
junk_records = output_df[output_df['Primary_Code'] == 'JUNK']

print(f"Records requiring manual review: {len(junk_records)}")
junk_records.to_csv('junk_for_review.csv', index=False)
```

---

## ğŸ”‘ Key Classes

### SpecialtyPreprocessor

Handles all text preprocessing operations.

```python
from src.preprocessor import SpecialtyPreprocessor

preprocessor = SpecialtyPreprocessor()
cleaned_text, is_compound = preprocessor.preprocess("Cardio Surgery")
# Output: ("cardiology surgery", False)
```

### SpecialtyMatcher

Implements multi-strategy matching logic.

```python
from src.matcher import SpecialtyMatcher

matcher = SpecialtyMatcher(nucc_df)
result = matcher.match("ENT Surgeon")
# Returns: MatchResult with code, confidence, method, and alternatives
```

### ProviderSpecialtyStandardizer

Main orchestrator class for end-to-end standardization.

```python
from src.standardizer import ProviderSpecialtyStandardizer

standardizer = ProviderSpecialtyStandardizer(nucc_df)
output_df = standardizer.standardize(input_df)
metrics = standardizer.compute_validation_metrics(output_df)
```

### ConfidenceCalibrator

Calibrates raw confidence scores to true probabilities.

```python
from src.calibrator import ConfidenceCalibrator

calibrator = ConfidenceCalibrator()
calibrator.fit(original_scores, ground_truth)
calibrated = calibrator.calibrate(new_scores)
```

---

## ğŸ“ˆ Performance Optimization

### Memory Efficiency
- Streaming processing for large datasets
- Batch embedding computation
- Sparse matrix support for similarity calculations

### Speed Optimization
- Cached preprocessor results
- Pre-computed NUCC embeddings
- Vectorized similarity calculations
- Early exit from matching cascade

### Scaling
- Process 1,000+ records per minute
- Handles 9,000+ NUCC codes
- GPU support for embeddings (CUDA-compatible)

---

## ğŸ“ Technical Details

### Medical Abbreviation Mappings (44 Total)

```
cardio â†’ cardiology
obgyn â†’ obstetrics and gynecology
neuro â†’ neurology
ent â†’ otolaryngology
surg â†’ surgery
derm â†’ dermatology
psych â†’ psychiatry
ortho â†’ orthopedics
pt â†’ physical therapy
[... and 35 more]
```

### Stopwords Removed (15 Total)

```
service, center, clinic, hospital, department,
medical, healthcare, provider, physician, doctor,
general, office, practice, specialty, specialization
```

### Common Misspellings Corrected (9 Total)

```
clinal â†’ clinical
cardiak â†’ cardiac
diabetus â†’ diabetes
ural â†’ urology
oncolog â†’ oncology
[... and 4 more]
```

---

## ğŸ¤ Contributing

We welcome contributions! Here's how to help:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Areas for Contribution

- [ ] Additional language support (Spanish, Hindi, etc.)
- [ ] Custom abbreviation additions
- [ ] Performance optimizations
- [ ] Additional evaluation metrics
- [ ] Integration with healthcare systems (HL7, FHIR)
- [ ] Web UI for manual verification

---

## ğŸ“‹ Requirements

```
pandas>=1.3.0
numpy>=1.21.0
rapidfuzz>=2.0.0
sentence-transformers>=2.2.0
torch>=1.9.0
scikit-learn>=1.0.0
```

### Optional
```
jupyter>=1.0.0          # For notebooks
pytest>=6.0.0           # For testing
pytest-cov>=2.12.0      # For coverage reports
```

---

## ğŸ“ Data Files

### Input File Format
Required: `input_specialties.csv` with column `raw_specialty`

```csv
raw_specialty
ACUPUNCTURE
ADOLESCENT MEDICINE
CARDIOLOGY
[...]
```

### NUCC Taxonomy Master
Required: `nucc_taxonomy_master.csv` with columns `Code` and `Display_Name`

```csv
Code,Display_Name
101Y00000X,Acupuncture
102L00000X,Adolescent Medicine
207RC0000X,Cardiovascular Disease
[...]
```

### Output Files Generated

1. **output_standardized_CORRECTED.csv** - Comprehensive technical output
2. **output_specialty_explain.csv** - Simplified business-friendly format

---

## ğŸ“Š Evaluation Metrics

### Core Metrics
- **Mapping Success Rate**: % of records successfully mapped (target: >90%)
- **Junk Rate**: % of unmappable records (target: <10%)
- **Average Confidence**: Mean calibrated confidence score (target: >0.75)

### Method-Specific Metrics
- Exact Match: 95%+ accuracy
- Fuzzy Match: 80%+ accuracy
- Semantic Match: 50%+ accuracy

### Quality Metrics
- Low-confidence records (<0.60): Track for manual review
- Multi-specialty detection: Compound input handling
- Alternative suggestions: Top-5 ranked alternatives

---

## ğŸ” Data Privacy & Security

- No data is sent to external services
- All embeddings computed locally
- HIPAA-compliant processing (no PHI storage)
- Audit trail for all standardization operations

---

## ğŸ† Acknowledgments

- **NUCC Taxonomy**: Data provided by American Medical Association (AMA) & CMS
- **HiLabs Hackathon 2025**: Challenge organizers
- **sentence-transformers**: Pre-trained embedding models
- **rapidfuzz**: Fuzzy string matching library

---



## ğŸ“§ Contact

**Authors**: Ashwani Singh, Ayush Dixit, Adhiraj 
**Email**: ashwaniks22@iitk.ac.in 

---

**Made with â¤ï¸ for healthcare data quality**

Last Updated: November 2025
